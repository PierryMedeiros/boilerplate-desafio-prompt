# Desafio Final: Otimizador de Prompts com Testes Automatizados

ğŸ¯ **Objetivo**

VocÃª atuarÃ¡ como um Prompt Engineer focado em qualidade. Sua missÃ£o Ã© garantir que o prompt principal seja robusto, testÃ¡vel e altamente eficaz, utilizando testes automatizados para validar a estrutura e mÃ©tricas de IA para validar o conteÃºdo.

VocÃª deve:

- Analisar o prompt "ruim" que estÃ¡ no arquivo `prompts/bug_to_user_story_v1.yml`, e no arquivo `prompts/bug_to_user_story_v2.yml` desenvolver o prompt novo.
- Criar Testes Automatizados para validar a estrutura e as regras de negÃ³cio do seu prompt.
- Otimizar o Prompt novo aplicando tÃ©cnicas avanÃ§adas (Few-shot, CoT, Role Playing) atÃ© passar nos testes.
- Publicar (Push) a versÃ£o otimizada no LangSmith.
- Atingir nota mÃ­nima de **0.9 (90%)** nas mÃ©tricas de avaliaÃ§Ã£o automÃ¡tica.

---

ğŸ–¥ï¸ **Exemplo de Fluxo no Terminal**

```bash
# 1. Publicar sua versÃ£o otimizada no Hub
python src/push_prompts.py

# 2. Avaliar a performance com mÃ©tricas de IA
python src/evaluate.py

# 3. Rodar seus testes (inicialmente vÃ£o falhar ou passar dependendo do seu progresso)
pytest tests/test_prompts.py -v
```

```text
Executando avaliaÃ§Ã£o dos prompts...
================================
Prompt: bug_to_user_story_v2
- Tone Score: 0.94
- Acceptance Criteria: 0.96
- Completeness: 0.93
================================
Status: APROVADO âœ“ - Todas as mÃ©tricas atingiram o mÃ­nimo de 0.9
```

---

ğŸ› ï¸ **Tecnologias & Ferramentas**

- **Linguagem:** Python 3.9+
- **Framework de Teste:** Pytest
- **Engenharia de Prompt:** LangChain & LangSmith Hub
- **Formato:** YAML (para estruturaÃ§Ã£o dos prompts)

---

ğŸ”‘ **ConfiguraÃ§Ã£o de Acesso (ObrigatÃ³rio)**

Para que os scripts funcionem, vocÃª precisarÃ¡ configurar as chaves de API no arquivo `.env`:

1.  **OpenAI (Recomendado)**
    - Crie uma API Key: [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)
    - Modelos usados: `gpt-4o-mini` (geraÃ§Ã£o) e `gpt-4o` (avaliaÃ§Ã£o)
    - Custo estimado: ~$1-3 USD

2.  **Gemini (OpÃ§Ã£o Gratuita)**
    - Crie uma API Key: [https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)
    - Modelos usados: `gemini-1.5-flash`
    - Limite: 15 req/min (pode haver lentidÃ£o na avaliaÃ§Ã£o em lote)

3.  **LangSmith (Plataforma de Prompts)**
    - Crie uma conta e uma API Key: [https://smith.langchain.com/](https://smith.langchain.com/)
    - NecessÃ¡rio para baixar o prompt base e subir sua versÃ£o final.

---

ğŸ“ **Passo a Passo do Desafio**

**1. ConfiguraÃ§Ã£o e Pull (Infraestrutura Pronta)**

O repositÃ³rio jÃ¡ contÃ©m os scripts necessÃ¡rios em `src/`. Sua primeira aÃ§Ã£o Ã© trazer o problema para sua mÃ¡quina.

**O que vocÃª deve fazer:**

- Fazer o fork e clone do repositÃ³rio.
- Instalar dependÃªncias: `pip install -r requirements.txt`.
- Configurar o `.env` com suas chaves.
- Executar:
  ```bash
  python src/pull_prompts.py
  ```
  Isso irÃ¡ baixar o conteÃºdo para `prompts/bug_to_user_story_v1.yml`.

**2. ImplementaÃ§Ã£o dos Testes (Sua tarefa de cÃ³digo)**

Antes de otimizar o prompt, vocÃª deve garantir que ele siga regras estritas. VocÃª escreverÃ¡ o cÃ³digo de teste que valida o seu prompt.

**O que vocÃª deve fazer:** Edite o arquivo `tests/test_prompts.py` e implemente, no mÃ­nimo, os 6 testes abaixo usando `pytest`:

- `test_prompt_has_system_prompt`: Verifica se o campo existe e nÃ£o estÃ¡ vazio.
- `test_prompt_has_role_definition`: Verifica se o prompt define uma persona (ex: "VocÃª Ã© um Product Manager").
- `test_prompt_mentions_format`: Verifica se o prompt exige formato Markdown ou User Story padrÃ£o.
- `test_prompt_has_few_shot_examples`: Verifica se o prompt contÃ©m exemplos de entrada/saÃ­da (tÃ©cnica Few-shot).
- `test_prompt_no_todos`: Garante que vocÃª nÃ£o esqueceu nenhum `[TODO]` no texto.
- `test_minimum_techniques`: Verifica (atravÃ©s dos metadados do yaml) se pelo menos 2 tÃ©cnicas foram listadas.

**Como validar:**

```bash
pytest tests/test_prompts.py
```

**3. OtimizaÃ§Ã£o do Prompt (Sua tarefa de Engenharia)**

Agora que os testes existem, vocÃª deve trabalhar na soluÃ§Ã£o.

**O que vocÃª deve fazer:**

- Analise o prompt ruim em `prompts/bug_to_user_story_v1.yml`.
- Edite o arquivo `prompts/bug_to_user_story_v2.yml` criando sua versÃ£o otimizada.
- Aplique pelo menos **duas** das seguintes tÃ©cnicas:
  - **Few-shot Learning:** Adicione exemplos reais de bugs -> user stories.
  - **Chain of Thought (CoT):** Instrua o modelo a pensar passo a passo antes de responder.
  - **Role Prompting:** Reforce a autoridade e o contexto da persona.
  - **Delimitadores:** Use marcaÃ§Ãµes claras para separar instruÃ§Ãµes de dados.
- Preencha o campo `techniques_applied` no YAML com as tÃ©cnicas que vocÃª usou.

*Dica: Use o arquivo `dataset.py` para ver exemplos de bugs que serÃ£o usados na avaliaÃ§Ã£o.*

**4. PublicaÃ§Ã£o e AvaliaÃ§Ã£o (Infraestrutura Pronta)**

Com o prompt otimizado e passando nos testes unitÃ¡rios, Ã© hora de ver como ele se sai contra mÃ©tricas de IA.

**O que vocÃª deve fazer:**

- Subir sua versÃ£o para o Hub:
  ```bash
  python src/push_prompts.py
  ```
- Rodar a avaliaÃ§Ã£o de qualidade:
  ```bash
  python src/evaluate.py
  ```

**CritÃ©rio de AprovaÃ§Ã£o:** VocÃª deve atingir uma nota mÃ©dia superior a **0.9** em todas as mÃ©tricas:

- **Tone Score:** O tom Ã© profissional?
- **Acceptance Criteria:** Gerou critÃ©rios de aceite vÃ¡lidos?
- **User Story Format:** Seguiu o padrÃ£o "Como um... Quero... Para..."?
- **Completeness:** NÃ£o perdeu nenhuma informaÃ§Ã£o do bug original?

*Caso a nota seja baixa, volte ao Passo 3, ajuste o prompt, faÃ§a o push novamente e reavalie.*

---

ğŸ“‚ **Estrutura do Projeto**

```text
desafio-prompt-engineer/
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ bug_to_user_story_v1.yml  # (Gerado pelo pull) Prompt ruim original
â”‚   â””â”€â”€ bug_to_user_story_v2.yml  # <--- VOCÃŠ EDITA ESTE ARQUIVO (O Prompt)
â”‚
â”œâ”€â”€ src/                          # (CÃ“DIGO PRONTO - NÃƒO EDITAR)
â”‚   â”œâ”€â”€ pull_prompts.py           # Script de download
â”‚   â”œâ”€â”€ push_prompts.py           # Script de upload
â”‚   â”œâ”€â”€ evaluate.py               # Script de avaliaÃ§Ã£o
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_prompts.py           # <--- VOCÃŠ EDITA ESTE ARQUIVO (Os Testes)
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

ğŸ“¦ **EntregÃ¡veis**

- **RepositÃ³rio GitHub** contendo:
  - Arquivo `tests/test_prompts.py` com os testes implementados.
  - Arquivo `prompts/bug_to_user_story_v2.yml` com o prompt otimizado.
  - `README.md` atualizado com a seÃ§Ã£o **"TÃ©cnicas Utilizadas"** explicando suas escolhas.

- **Link do LangSmith Hub:**
  - O script `push_prompts.py` vai gerar um link pÃºblico do seu prompt. Inclua-o no `README.md`.

- **EvidÃªncia de ExecuÃ§Ã£o:**
  - Screenshot do terminal mostrando os testes passando (`pytest`).
  - Screenshot do terminal mostrando as notas da avaliaÃ§Ã£o (`evaluate.py`) acima de 0.9.

Boa sorte! Transforme bugs caÃ³ticos em User Stories impecÃ¡veis! ğŸš€